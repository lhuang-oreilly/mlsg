{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last sessions, we talked about what it means for a supervised algorithm to *learn*. We saw that, in a supervised learning algorithm, we have an **input** with a corresponding **output** label. The goal is to learn the weights for given features that will give us the correct output - this is what it means to train a model.\n",
    "\n",
    "In this exercise, we will apply what we learned to predict the rating of a review from reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `train.csv` file, there are a small number of reviews; for each review, there is the text of the review, the number of people who have read the review, and the numerical score of the review:\n",
    "\n",
    "```\n",
    "\"This video is incredible!!!\",1,5.0\n",
    "\"Sad and sorry excuse for words on a screen\",8,1.5\n",
    "\"The instructor was amazing, incredible in every way!\",2,5.0\n",
    "\"I feel sorry for anyone who waste their time on this book.\",9,1.0\n",
    "\"This is so-so, not bad but not good either.\",6,3.0\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in every NLP pipeline is cleaning and tokenization. The first step is to extract the vocabulary and count statistics. \n",
    "\n",
    "In real life, you will have to decide what constitutes a **token**. Here, let's do a simple definition. A token is:\n",
    "* one single valid English word\n",
    "* lowercased\n",
    "* plurals are stemmed to their singular form\n",
    "* excludes stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# Let's try a max vocab size of 12 for now, and see how this works\n",
    "MAX_VOCAB_SIZE = 12\n",
    "PUNCTUATION = '.!-:'\n",
    "STOP_WORDS = set(PUNCTUATION) | {\n",
    "    '', ' ', \n",
    "    'a', 'an', 'the', \n",
    "    'for', 'on', 'of',\n",
    "    'and', 'or', \n",
    "    'is', 'was', 'be', \n",
    "    'has', 'have', 'had',\n",
    "    'this', 'that', 'these', 'those', 'their'\n",
    "}\n",
    "\n",
    "def silly_stemmer(word):\n",
    "    word = word.lower()\n",
    "    if word.endswith('es'):\n",
    "        return word[:-2]\n",
    "    if word.endswith('s'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def limit_vocabulary(vocabulary):\n",
    "    return [v[0] for v in vocabulary.most_common(MAX_VOCAB_SIZE)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "PUNCTUATION_TABLE = str.maketrans(PUNCTUATION, ' ' * len(PUNCTUATION))\n",
    "\n",
    "def word_vector(review):\n",
    "    v = Counter()\n",
    "    tokens = review.translate(PUNCTUATION_TABLE).split(' ')\n",
    "    v.update(silly_stemmer(token) for token in tokens if token not in STOP_WORDS)\n",
    "    return dict(v)\n",
    "\n",
    "def get_vocabulary(reviews):\n",
    "    vocabulary = Counter()\n",
    "    for review in reviews:\n",
    "        vocabulary.update(word_vector(review))\n",
    "    return limit_vocabulary(vocabulary)\n",
    "\n",
    "assert set(get_vocabulary([])) == set()\n",
    "assert set(get_vocabulary([''])) == set()\n",
    "assert set(get_vocabulary(['hello'])) == {'hello'}\n",
    "assert (set(get_vocabulary(['foo bar', 'bar baz hello', 'hello world'])) == \n",
    "        {'bar', 'foo', 'baz', 'hello', 'world'})\n",
    "assert (set(get_vocabulary(['foo - bar', 'bar: baz hello!', 'hello world.'])) == \n",
    "        {'foo', 'bar', 'baz', 'hello', 'world'})\n",
    "assert (set(get_vocabulary(['this is an awesome thing that has ever graced the movies'])) ==\n",
    "        {'awesome', 'ever', 'graced', 'movi', 'thing'})\n",
    "\n",
    "print('all good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and excluding stop words is tedious work. **Question**: What libraries might you use that can help you with this? Do some googling and write down below at least two libraries and function calls you can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: what are two libraries and their functions I could call to make this easier...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert each sample into a *feature vector*. In real life, you will decide what features to use, and in the future, we will use deep learning to help us decide features. But for now, we will use these features:\n",
    "* Counts of each token\n",
    "* The number of people who have read the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar', 'hello', 'foo', 'world', 'baz']\n",
      "[42, 0, 1, 0, 3, 0]\n",
      "[42, 0, 1, 0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "def vectorize(data, vocab):\n",
    "    '''\n",
    "        Given a sample and the vocabulary dictionary, convert the sample to a feature vector. \n",
    "        For example, given \"I love how incredible this was.\",2,4.5 \n",
    "        it should return a vector of counts of each vocab word and the # of people who have read the review\n",
    "    '''\n",
    "    review, readers, *_ = data\n",
    "    counts = word_vector(review)\n",
    "    return [readers,] + [counts.get(word, 0) for word in vocab]\n",
    "    \n",
    "vocab = get_vocabulary(['foo bar', 'bar baz hello', 'hello world'])\n",
    "print(vocab)\n",
    "print(vectorize(['hello world world world', 42, 4.5], vocab))\n",
    "print(vectorize(['hello world world baz', 42, 4.5], vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, vectorizing by hand can be tough work. **Question**: How might we be able to use sklearn to help us out here? Do some googling and name the class we could use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: what is the general class in sklearn that helps us with converting data to feature vectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call your two functions to get the vocabulary and vectorize the data into feature vectors. You should end up with $X$, the array of feature vectors for each sample, and $y$, the array of truth labels (in this case, the review score) for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality is 12 + 1\n",
      "Vocabulary:\n",
      "['not', 'great', 'i', 'so', 'waste', 'book', 'time', 'very', 'learning', 'excuse', 'video', 'screen']\n",
      "[['1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0']\n",
      " ['8' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '1']\n",
      " ['2' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['9' '0' '0' '1' '0' '1' '1' '1' '0' '0' '0' '0' '0']\n",
      " ['6' '2' '0' '0' '2' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['3' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0']\n",
      " ['7' '0' '0' '1' '1' '0' '0' '0' '0' '0' '1' '0' '0']\n",
      " ['2' '0' '1' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0']\n",
      " ['1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0']\n",
      " ['6' '0' '0' '0' '0' '2' '0' '1' '0' '0' '0' '0' '0']\n",
      " ['6' '1' '0' '0' '0' '0' '0' '0' '1' '0' '0' '1' '1']\n",
      " ['8' '1' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0']\n",
      " ['7' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['3' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['1' '0' '3' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0']]\n",
      "[ 5.   1.5  5.   1.   3.   4.5  2.5  4.   5.   2.   2.   2.   1.5  4.5  5. ]\n"
     ]
    }
   ],
   "source": [
    "# call the get_vocabulary() and vectorize() functions\n",
    "import csv\n",
    "\n",
    "with open('./train.csv', 'r') as train_f:\n",
    "    reader = csv.reader(train_f)\n",
    "    reviews = [r for r in reader]\n",
    "    \n",
    "# process your file here, call your functions, etc.\n",
    "vocab = get_vocabulary([review[0] for review in reviews])\n",
    "\n",
    "X = np.asarray([vectorize(review, vocab) for review in reviews])\n",
    "y = np.asarray([float(review[-1]) for review in reviews])\n",
    "\n",
    "# make sure they're numpy array objects.\n",
    "\n",
    "assert isinstance(X, np.ndarray) == True\n",
    "assert isinstance(y, np.ndarray) == True\n",
    "print(\"Dimensionality is\", len(vocab), \"+ 1\")\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How many features do we have? In other words, what is the dimensionality of our training set?\n",
    "\n",
    "**TODO: Type in your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
